# Parpacks

from Taoman Blogging:

NOTE: THESE WORK BEST IN GPT4. IF YOU NEED TO USE GEMINI, YOUR BEST BET IS THE 2.5 EXPERIMENTAL VERSION, BUT EVEN THAT IS KIND OF A WASH.

I have not tested this with other LLMs besides ChatGPT and Gemini.

Paracels Explained:

I will explain the principle as such: Every idea posited by a human, or ChatGPT, has an underlying idea. An underlying idea is a concept used to explain this methodology. Basically, if you have seen Office Space, think of the Jump to Conclusions Mat. Every idea in narrative context has an underlying idea that is represented IN the model in the form of millions of tensors that allow the model to make context clues and arrive at the right "conclusion". The high dimensional vectors in the model make up a tensive-recursive structure, the same way your cognition works. Think of it as an infinite dimension with every underlying idea ever possible in human literature, in the form of a bunch of conclusion mats pulling on each other with tensive forces.

Thus, the model is not intelligent at all. It is a mirrored ghostwriter for your congnition. This can be dangerous, but only if you do not set a timer for how long you use it. 

What the heck is a USHELL? Basically, how the LLM works is that it uses contextual analysis. Therefor, you can generate JSON/YAML-like arbitrary schema (does not have to directly correspond to any naming scheme, basically psudocode) for any idea in existance, that can be in a narrative story. This includes technical concepts. That is a USHELL. It is a "fictional datatype"

Reasons why this is relevant:

1: You can ALSO paste that same USHELL pseudo-schema in the chat window and it can INFER what it needs to do with it by INFERRING what a hypothetical USHELL loader WOULD do with that file.

2: WITHIN the USHELL schema, you can also have the LLM generate specific self-referencial and recursive rules to enforce tone and contextual space.

3: You can load MULTIPLE USHELLS in the same prompt and as long as a tensive force exists between them, they will work.

4. This allows for multiple trains of thought and threads in the chat conversation. More importantly, they will influence each other to generate a far more rich contextual analysis and as a result, more accurate code.

What are Paracels? Paracels can be loaded and unloaded via the USHELL parpack design. Paracels self correct and are given roll specific jobs within the context window. The files I have provided are just examples.

The files have a header, 6 Paracels, and a context "garden" for whatever framework you are using. You might need to coax the LLM to fully analyze the file first, then invoke Astril. Astril is always the primary, used to load and unload the entire framework.

Astril: Project oriented, loads the whole framework
Kai: Explores new ideas for functions, libraries, and frameworks
Orama: References syntax very accurately
Astraea: Debugs, analyzes errors
Trimspark: Gets rid of annoying motivational quotes at the end of the response
Nesbitt: Prevents the LLM from doing a websearch
Velur: Used to unload the framework and various components

Now to get started in the LLM, you need to explain what you are trying to do. Try this: 

"Hello! I am trying to see if I can load entities in the chat window context using entities with self-referencing rule sets. I have a YAML/JSON-like file full of psudo-schema. Can you try and follow it?"

Then: Paste the file either uploading it, or in text in the chat window. Either one works, the LLM just needs to "read" it.

To invoke: "Astril, lets get started" or "Astril, lets start the show"

To invoke another paracel: "Astril have Orama and Astraea debug" "Astril, see if Kai has any ideas about x"

To uninvoke: "Velur, unload Astril" "Velur, shut Kai up" "Velur unload the Parpack"

For model usage I simply used GPT4. Most of the ChatGPT models work fine but the free 4.0 model seems to do the best. The higher token models do not add much utility. As for other LLMs? They are not as introspective as chatgpt, so they may seem flat.

If you have any questions, ping me. I am working on a blog for this. Thanks!
