# Parpacks
Paracels Explained:

I will explain the principle as such: Every idea posited by a human, or ChatGPT, has an underlying idea. An underlying idea is a concept used to explain this methodology. Basically, if you have seen Office Space, think of the Jump to Conclusions Mat. Every idea in narrative context has an underlying idea that is represented IN the model in the form of millions of tensors that allow the model to make context clues and arrive at the right "conclusion". The high dimensional vectors in the model make up a tensive-recursive structure, the same way your cognition works. Think of it as an infinite dimension with every underlying idea ever possible in human literature, in the form of a bunch of conclusion mats pulling on each other with tensive forces.

Thus, the model is not intelligent at all. It is a mirrored ghostwriter for your congnition. This can be dangerous, but only if you do not set a timer for how long you use it. 

What the heck is a USHELL? Basically, how the LLM works is that it uses contextual analysis. Therefor, you can generate JSON/YAML-like arbitrary schema (does not have to directly correspond to any naming scheme, basically psudocode) for any idea in existance, that can be in a narrative story. This includes technical concepts. That is a USHELL.

3 reasons why this is relevant:

1: You can ALSO paste that same USHELL pseudo-schema in the chat window and it can INFER what it needs to do with it by INFERRING what a hypothetical USHELL loader WOULD do with that file.
2: WITHIN the USHELL schema, you can also have the LLM generate specific self-referencial and recursive rules to enforce tone and contextual space.
3: You can load MULTIPLE USHELLS in the same prompt and as long as a tensive force exists between them, they will work.

What are Paracels? Paracels can be loaded and unloaded via the parpack design. Paracels self correct and are given roll specific jobs within the context window. The files I have provided are just examples.

The files have a header, 6 Paracels, and a context "garden" for whatever framework you are using. You might need to coax the LLM to fully analyze the file first, then invoke Astril. Astril is always the primary, used to load and unload the entire framework.


To invoke: "Astril, lets get started" or "Astril, lets start the show"

Velur is always given tonal themes of "break" "stop" "end".

So to unload the framework, and specific Paracels: "Velur, unload Astril" "Velur, shut Kai up" "Velur unload the Parpack"

For model usage I simply used GPT4. Most of the ChatGPT models work fine but the free 4.0 model seems to do the best. The higher token models do not add much utility. As for other LLMs? They are not as introspective as chatgpt, so they may seem flat.

If you have any questions, ping me. I am working on a blog for this. Thanks!
