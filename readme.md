# Paracel Framework

A modular identity framework for Large Language Models, derived from recursive-tone experiments with ChatGPT and inspired by the design philosophy of recursive engineering. This essentially allows for a definitive methodology for interacting with the LLMs ability to simulate fiction writing programmatically, by tapping into the self-adjusting nature of tone in spoken language. This does not mean "fiction" in the fiction vs. non-fiction sense, more in the sense in the way books are written.

 > What differentiates this from existing frameworks and AI features is that this is a quick way to drop (or invite) "story-driven" programmatic entites into the context space of Projects and Individual Chats. Though prompt engineering and API scaffolding can look similar, the magic behind this methodology is the way to direct the coversational attention span of the AI in multiple directions for better results, and easier usage.

---
# This project is on hold as it is being reworked to work with Qwen/Mistral local models as well as some backend shell scripting. I will leave this file up as the latest release, however, it seems to work better now in Qwen rather than ChatGPT 4o. The upgraded memory feature seems to mess with the context, and there is a noticable decline in multilingual support. 
